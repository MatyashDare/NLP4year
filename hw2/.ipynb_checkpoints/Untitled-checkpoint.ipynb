{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2. Извлечение коллокаций + NER\n",
    "\n",
    "## Описание\n",
    "\n",
    "Выберите корпус отзывов на товары одной из категорий Amazon: http://jmcauley.ucsd.edu/data/amazon/\n",
    "\n",
    "(В низу страницы по ссылке есть код для загрузки данных, можете им воспользоваться)\n",
    "\n",
    "Допустим, что вам нужно подготовить аналитический отчет по этим отзывам — например, для производителя нового продукта этой категории. Для этого будем искать упоминания товаров в отзывах (будем считать их NE). Учтите, что упоминание может выглядеть не только как \"Iphone 10\", но и как \"модель\", \"телефон\" и т.п.\n",
    "\n",
    "Важное замечание: в задании приводятся примеры решений, вы можете их использовать!\n",
    "\n",
    "**1. (3 балла)** Предложите 3 способа найти упоминания товаров в отзывах. Например, использовать bootstrapping: составить шаблоны вида \"холодильник XXX\", найти все соответствующие n-граммы и выделить из них называние товара. Могут помочь заголовки и дополнительные данные с Amazon (Metadata здесь) Какие данные необходимы для каждого из способов? Какие есть достоинства/недостатки?\n",
    "\n",
    "**2. (2 балла)** Реализуйте один из предложенных вами способов.\n",
    "\n",
    "Примеры в качестве подсказки (можно использовать один из них):\n",
    "\n",
    "написать правила с помощью natasha/yargy\n",
    "составить мини-словарь сущностей/дескрипторов, расширить с помощью эмбеддингов (например, word2vec)\n",
    "\n",
    "\n",
    "**3. (1 балл)** Соберите n-граммы с полученными сущностями (NE + левый сосед / NE + правый сосед)\n",
    "\n",
    "**4. (3 балла)** Ранжируйте n-граммы с помощью 3 коллокационных метрик (t-score, PMI и т.д.). Не забудьте про частотный фильтр / сглаживание. Выберите лучший результат (какая метрика ранжирует выше коллокации, подходящие для отчёта).\n",
    "\n",
    "**5. (1 балл)** Сгруппируйте полученные коллокации по NE, выведите примеры для 5 товаров. Должны получиться примерно такие группы:\n",
    "\n",
    "watch \n",
    "\n",
    "\n",
    "stylish watch, \n",
    "good watches, \n",
    "great watch, \n",
    "love this watch\n",
    "...\n",
    "\n",
    "**Бонус (2 балла):** если придумаете способ объединить синонимичные упоминания (например, \"Samsung Galaxy Watch\", \"watch\", \"smartwatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Предложение способов найти упоминания товаров в отзывах.\n",
    "\n",
    "## 1.1\n",
    "\n",
    "Можно выделить самые частотные слова (по абсолютной частоте встречаемости) в названиях товаров и их категории (у нас достаточно много описаний с проставленной категорией). Так мы можем (скорее всего!) выделить названия товаров. Так будет точно меньше \"ручной\" работы написания \"нужных\" слов - автоматизация процесса всегда приятнее.\n",
    "\n",
    "Когда получим \"базовые\" названия товаров, можно брать 1 или 2 соседа справа, слева, взять би- и три- граммы с \"нужным\" словом и взять только такие словосочетания, которые встречаются и в названии товаров - так можно отфильтровать \"шум\", который остался бы после фильтрации просто по частотности, без учета вхождения в название товара\n",
    "\n",
    "## 1.2\n",
    "\n",
    "Выделять самые \"важные\" слова можно и векторайзерами, например,  из просто TFidfVectorizer-а (или как-нибудь еще, это просто к примеру), сделать опять фильтрацию по какому-нибудь порогу частоты встречаемости слова и, может (это будет логичнее и \"безопаснее\" всего), сделать еще фильрацию по части речи - оставить существительные. Поэтому вряд ли \"захватим\" таким способом еще и бренды/марки товаров, но что уж тут поделать? Можно поставить \"костыль\" с поиском самых частотных слов, встречаемых с отобранными существительными.\n",
    "## 1.3\n",
    "\n",
    "Никогда не плохо использовать морфологические шаблоны, для этого можно проанализировать, как вообще обычно называют товары (спойлер: обычно NOUN + NOUN, ADJ + NOUN (такой порядок в английском языке), ADJ + NOUN + NOUN), а потом \"повытаскивать\" из отзывов такие конструкции. В этом подходе есть очевидный недостаток: всегда точно попадутся словосочетания, не соотвествующие цели задания. Но можно поставить \"костыль\" - например, проверять вхождения слова категории в найденном словосочетании (например, есть ли в выделенной паре слов слово \"пылесос\"). \n",
    "\n",
    "## 1.4\n",
    "\n",
    "Еще можно \"подкрутить\" модель, способную распознавать NER-ы, в частности, нас будут интересовать случаи, когда модель сможет находить названия компании - тег ORG, и брать 1 или 2 соседа справа, слева, и, может, по частоте таких би- и три-грамм вычленять NER + NOUN, NOUN + NER самые частые случаи. Тут есть недостаток опять же в том, что не факт, что попадется только то, что нам нужно, но \"костыль\" с частотностью и дополнительной проверкой на вхождение какого-нибудь слова из списка \"нужных\" слов (т.е. слов, обозначающие какие-либо бытовые приборы и тд) может частично поправить ситуацию.\n",
    "\n",
    "\n",
    "\n",
    "**Примечание:** каждый из предложенных способов можно улучшить с помощью  расширения \"нужных\" словосочетаний с помощью перебора синонимов с word2vec-моделью (например), обученной только на нашем корпусе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Реализация одного из предложенных способов.\n",
    "\n",
    "Для начала импортируем все, что нужно\n",
    "\n",
    "Здесь же я создаю словарь стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import pymorphy2\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.extend('img alt src https media amazon com images aplus media vc   d cb  __ jpg _sl300__'.split())\n",
    "stopwords_list.extend(['img', 'alt', 'src', 'https', 'media', 'amazon',\n",
    "                       'com', 'images', 'aplus', 'media', 'vc', 'jpg', 'class', 'spacing', 'mini', 'br'])\n",
    "stopwords_list.extend(['link', 'normal', 'target', '_blank', 'rel', 'noopener', 'href', 'png', 'jpeg',\n",
    "                      'em', 'bottom', 'line','bfdb'\n",
    "                      'div', 'text', 'center', 'view', 'larger', 'div'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Home_and_Kitchen.json.gz\n",
    "# !wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Luxury_Beauty.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1Zv6MARGQcrBbLHyjPVVMZVnRWsRnVMpV\n",
    "\n",
    "\n",
    "https://colab.research.google.com/drive/12r4KJVbNqjjhiZ6aeiaG809x4-Tg5fm8?usp=sharing#scrollTo=EdaX7BmI8OW_\n",
    "\n",
    "https://nijianmo.github.io/amazon/index.html\n",
    "\n",
    "https://nijianmo.github.io/amazon/index.html#files\n",
    "\n",
    "https://nijianmo.github.io/amazon/index.html#code\n",
    "\n",
    "\n",
    "https://github.com/named-entity/hse-nlp/blob/master/4th_year/hw/hw2.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9666it [00:02, 4082.41it/s]\n"
     ]
    }
   ],
   "source": [
    "meta_data = []\n",
    "\n",
    "i = 0\n",
    "with gzip.open('meta_Home_and_Kitchen.json.gz', 'r') as f:\n",
    "# with open('meta_Home_and_Kitchen.json') as f:\n",
    "    for l in tqdm(f):\n",
    "        if i < 10000:\n",
    "            meta_data.append(json.loads(l))\n",
    "            i += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41047it [00:00, 82371.93it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "i = 0\n",
    "with open('sample_Home_and_Kitchen_5.json') as f:\n",
    "    for l in tqdm(f):\n",
    "        if i < 10000:\n",
    "            data.append(json.loads(l))\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': 4.0,\n",
       " 'verified': True,\n",
       " 'reviewTime': '08 3, 2014',\n",
       " 'reviewerID': 'A2LVMEG7IP5F90',\n",
       " 'asin': 'B00002N62Y',\n",
       " 'reviewerName': 'jeff',\n",
       " 'reviewText': 'good product and price',\n",
       " 'summary': 'Four Stars',\n",
       " 'unixReviewTime': 1407024000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41047/41047 [00:00<00:00, 504257.50it/s]\n"
     ]
    }
   ],
   "source": [
    "reviewText = []\n",
    "asin = []\n",
    "progress_bar = tqdm(total=len(data))\n",
    "for i,d in enumerate(data):\n",
    "    if 'reviewText' in d.keys():\n",
    "        reviewText.append(d['reviewText'])\n",
    "        asin.append(d['asin'])\n",
    "    progress_bar.update(1)\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good product and price</td>\n",
       "      <td>B00002N62Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These belts are $2+ retail and some retailers/...</td>\n",
       "      <td>B00002N62Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These came in a 2 pk and are perfect fit for m...</td>\n",
       "      <td>B00002N62Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText        asin\n",
       "0                             good product and price  B00002N62Y\n",
       "1  These belts are $2+ retail and some retailers/...  B00002N62Y\n",
       "2  These came in a 2 pk and are perfect fit for m...  B00002N62Y"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'reviewText': reviewText, 'asin': asin}).head(10000)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 35358.81it/s]\n"
     ]
    }
   ],
   "source": [
    "description = []\n",
    "title = []\n",
    "category = []\n",
    "brand = []\n",
    "category_correct = []\n",
    "categories_to_count = []\n",
    "asin2 = []\n",
    "\n",
    "def all_categories(list_cat):\n",
    "    ans = []\n",
    "    for cat in list_cat:\n",
    "        if '&' in cat:\n",
    "            ans.extend(cat.split(' & '))\n",
    "        else:\n",
    "            ans.append(cat)\n",
    "    return ans\n",
    "\n",
    "\n",
    "progress_bar = tqdm(total=len(meta_data))\n",
    "for i,d in enumerate(meta_data):\n",
    "    # описания товаров лежат списком из предложений - я сохраню все строчкой\n",
    "    if 'description' in d.keys():\n",
    "        description.append(' '.join(d['description']))\n",
    "        title.append(d['title'])\n",
    "        if d['category']:\n",
    "        # берем все элементы списка, кроме первого - тк там \n",
    "        # лежит \"главная\" категория - Home nad Kitchen\n",
    "            category.append(d['category'][1:])\n",
    "            cat = all_categories(d['category'][1:])\n",
    "            category_correct.append(set(cat))\n",
    "            categories_to_count.extend(list(set(cat)))\n",
    "        else:\n",
    "            category.append('no category')\n",
    "        if 'brand' in d.keys():\n",
    "            brand.append(d['brand'])\n",
    "        else:\n",
    "            brand.append('no brand')\n",
    "\n",
    "        asin2.append(d['asin'])\n",
    "    progress_bar.update(1)\n",
    "progress_bar.close()\n",
    "\n",
    "\n",
    "\n",
    "meta = pd.DataFrame({'description': description,\n",
    "                   'category': category,\n",
    "                   'category_correct': category_correct,\n",
    "                  'title': title,\n",
    "                  'brand': brand,\n",
    "                'asin': asin2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>category_correct</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It was a time honored tradition among the earl...</td>\n",
       "      <td>[Kitchen &amp; Dining, Dining &amp; Entertaining, Dinn...</td>\n",
       "      <td>{Kitchen, Plates, Dinner Plates, Dining, Enter...</td>\n",
       "      <td>You Are Special Today Red Plate [With Red Pen]</td>\n",
       "      <td>Waechtersbach USA</td>\n",
       "      <td>0001487795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VICKS INHALER relieves stuffy noses helps sinu...</td>\n",
       "      <td>[Home Dcor, Candles &amp; Holders, Candles]</td>\n",
       "      <td>{Holders, Candles, Home Dcor}</td>\n",
       "      <td>Vicks Inhaler Relief for Cold Sinus Nasal Cong...</td>\n",
       "      <td>Vicks</td>\n",
       "      <td>0002020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16 oz squeeze bottle, 1 lb.</td>\n",
       "      <td>[Kitchen &amp; Dining, Dining &amp; Entertaining, Glas...</td>\n",
       "      <td>{Drinkware, Kitchen, Wine, Champagne Glasses, ...</td>\n",
       "      <td>Artistic Churchware Communion Cup Filler: RW525</td>\n",
       "      <td>Artistic Churchware</td>\n",
       "      <td>0006564224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The only soap in the world with a unique combi...</td>\n",
       "      <td>[Bath, Bathroom Accessories]</td>\n",
       "      <td>{Bathroom Accessories, Bath}</td>\n",
       "      <td>4 BARS! Mysore Sandal Soap 70grams FAST SHIPPING</td>\n",
       "      <td>Mysore</td>\n",
       "      <td>0009046461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Divya Arogya vati improves health, It has natu...</td>\n",
       "      <td>[Home Dcor, Home Fragrance, Incense &amp; Incense ...</td>\n",
       "      <td>{Home Fragrance, Incense Holders, Home Dcor, I...</td>\n",
       "      <td>AROGYA VATI (40gm) by popeye seller</td>\n",
       "      <td>Patanjali</td>\n",
       "      <td>0234937912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  It was a time honored tradition among the earl...   \n",
       "1  VICKS INHALER relieves stuffy noses helps sinu...   \n",
       "2                        16 oz squeeze bottle, 1 lb.   \n",
       "3  The only soap in the world with a unique combi...   \n",
       "4  Divya Arogya vati improves health, It has natu...   \n",
       "\n",
       "                                            category  \\\n",
       "0  [Kitchen & Dining, Dining & Entertaining, Dinn...   \n",
       "1            [Home Dcor, Candles & Holders, Candles]   \n",
       "2  [Kitchen & Dining, Dining & Entertaining, Glas...   \n",
       "3                       [Bath, Bathroom Accessories]   \n",
       "4  [Home Dcor, Home Fragrance, Incense & Incense ...   \n",
       "\n",
       "                                    category_correct  \\\n",
       "0  {Kitchen, Plates, Dinner Plates, Dining, Enter...   \n",
       "1                      {Holders, Candles, Home Dcor}   \n",
       "2  {Drinkware, Kitchen, Wine, Champagne Glasses, ...   \n",
       "3                       {Bathroom Accessories, Bath}   \n",
       "4  {Home Fragrance, Incense Holders, Home Dcor, I...   \n",
       "\n",
       "                                               title                brand  \\\n",
       "0     You Are Special Today Red Plate [With Red Pen]    Waechtersbach USA   \n",
       "1  Vicks Inhaler Relief for Cold Sinus Nasal Cong...                Vicks   \n",
       "2    Artistic Churchware Communion Cup Filler: RW525  Artistic Churchware   \n",
       "3   4 BARS! Mysore Sandal Soap 70grams FAST SHIPPING               Mysore   \n",
       "4                AROGYA VATI (40gm) by popeye seller            Patanjali   \n",
       "\n",
       "         asin  \n",
       "0  0001487795  \n",
       "1  0002020300  \n",
       "2  0006564224  \n",
       "3  0009046461  \n",
       "4  0234937912  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_Counter = Counter(categories_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.7314990512334"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(categories_Counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10\n",
    "categories_Counter_filtered = {key: value for key,value in categories_Counter.items() if value > threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect for cheesecake and other cake baking, this Kaiser Bakeware Noblesse Nonstick Springform Pan features a heavy gauge steel for even heat distribution and consistent browning, while the non-stick interior eliminates the need for flouring pans. The buckle allows you to remove the exterior of the pan without compromising the cake shape, rendering a perfect circumference every time. This leakproof 10-inch springform pan is ideal for that treasured family cheesecake recipe. The pan's walls are reversible for both right- and left-handed use, and its superior buckle has a lifetime warranty. The perfectly flat bottom is easy to release and provides an evenly baked dessert. For optimal cooking, coat the interior surface of the pan with a very thin layer of butter before each use--this will guarantee a perfect release of cakes and breads. Because of this pan's heavy-gauge steel construction and dark-colored coating, goods baked in it will generally finish cooking in about 10% less time than specified in your recipe. <I>--Julie McCowan</I> \n",
      " Kaiser Bakeware Noblesse 10-Inch Nonstick Springform Pan\n"
     ]
    }
   ],
   "source": [
    "print(meta['description'][1000], '\\n', meta['title'][1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Kenmore canister vacuum broke down after only a few years, and we were pretty frustrated. We figured that if every vacuum we buy is going to break down after a year or two, we might as well buy something more affordable! So far, I think this vacuum is working well for us. (I'll update if it breaks down).\n",
      "\n",
      "So things to know: it does look cheap. The plastic parts are clunky. But, it is light-weight and easy to carry around the house, and it seems to be picking up everything efficiently. I was able to maneuver it under furniture more easily than the canister. Unlike other reviewers, I don't find it to be particularly noisy (at least, no more so then the Kenmore that cost 4x as much!).  It also did a good job on cat hair in high usage areas.\n"
     ]
    }
   ],
   "source": [
    "print(df['reviewText'][1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'are', 'you', 'mwlkdj', '78231']\n",
      "['hello', 'how', 'are', 'you', 'mwlkdj', '78231']\n"
     ]
    }
   ],
   "source": [
    "def bt(token_pattern=r'(?u)\\b\\w\\w+\\b'):\n",
    "    return re.compile(token_pattern).findall\n",
    "tokenizer=bt()\n",
    "print(tokenizer('hello how are you ?.mwlkdj----- +78231'))\n",
    " \n",
    "def tokenize_line(line: str):\n",
    "    if isinstance(line, str):\n",
    "        return [w.lower() for w in tokenizer(line)]\n",
    "    else:\n",
    "        return 'нет текста'\n",
    "print(tokenize_line('hello how are you ?.mwlkdj----- +78231'))\n",
    " \n",
    "\n",
    "# class Lemmatizer:\n",
    "#     def __init__(self):\n",
    "#         self.morph = pymorphy2.MorphAnalyzer()\n",
    "      \n",
    "#     def __call__(self, x: str) -> str:\n",
    "#         lemma = self.morph.parse(x)[0].normal_form\n",
    "#         return lemma\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "def lemmatize_df(df, text_column):\n",
    "    unique_toks = set([])\n",
    "    for line in tqdm(df[text_column]):\n",
    "        for w in line:\n",
    "            if w not in unique_toks:\n",
    "                unique_toks.add(w)\n",
    "    print('made unique tokens set')\n",
    "    t_l = {}\n",
    "    for ut in unique_toks:\n",
    "        t_l[ut] = lemmatizer.lemmatize(ut)\n",
    "    print('made tokens-lemmas dict')\n",
    "    return unique_toks, t_l\n",
    " \n",
    "def lemmatize_line(t_l, line):\n",
    "    return [t_l[w] for w in line if w in t_l]\n",
    "\n",
    "def clean_lemmas(line):\n",
    "    ans = []\n",
    "    for w in line:\n",
    "        if re.findall('_\\w\\w[0-9]*__', w) == []:\n",
    "            if re.findall('\\d', w) == []:\n",
    "                if w not in stopwords_list:\n",
    "                    ans.append(w)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bee'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('bees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 103916.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made unique tokens set\n",
      "made tokens-lemmas dict\n"
     ]
    }
   ],
   "source": [
    "df['tokens'] = df['reviewText'].apply(tokenize_line)\n",
    "unique_toks_df, t_l_df = lemmatize_df(df, 'tokens')\n",
    "df['lemmas'] = df['tokens'].apply(lambda x: lemmatize_line(t_l_df, x))\n",
    "df['cleaned_lemmas'] = df['lemmas'].apply(clean_lemmas)\n",
    "df['cleaned_lemmas_str'] = df['cleaned_lemmas'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>asin</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>cleaned_lemmas</th>\n",
       "      <th>cleaned_lemmas_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good product and price</td>\n",
       "      <td>B00002N62Y</td>\n",
       "      <td>[good, product, and, price]</td>\n",
       "      <td>[good, product, and, price]</td>\n",
       "      <td>[good, product, price]</td>\n",
       "      <td>good product price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These belts are $2+ retail and some retailers/...</td>\n",
       "      <td>B00002N62Y</td>\n",
       "      <td>[these, belts, are, retail, and, some, retaile...</td>\n",
       "      <td>[these, belt, are, retail, and, some, retailer...</td>\n",
       "      <td>[belt, retail, retailer, seller, use, generic,...</td>\n",
       "      <td>belt retail retailer seller use generic non oe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These came in a 2 pk and are perfect fit for m...</td>\n",
       "      <td>B00002N62Y</td>\n",
       "      <td>[these, came, in, pk, and, are, perfect, fit, ...</td>\n",
       "      <td>[these, came, in, pk, and, are, perfect, fit, ...</td>\n",
       "      <td>[came, pk, perfect, fit, good, old, vac, seem,...</td>\n",
       "      <td>came pk perfect fit good old vac seem strong d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So my super fancy vacuum cleaner, the one with...</td>\n",
       "      <td>B00002N62Y</td>\n",
       "      <td>[so, my, super, fancy, vacuum, cleaner, the, o...</td>\n",
       "      <td>[so, my, super, fancy, vacuum, cleaner, the, o...</td>\n",
       "      <td>[super, fancy, vacuum, cleaner, one, cold, fus...</td>\n",
       "      <td>super fancy vacuum cleaner one cold fusion rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Right one for my eureka.</td>\n",
       "      <td>B00002N62Y</td>\n",
       "      <td>[right, one, for, my, eureka]</td>\n",
       "      <td>[right, one, for, my, eureka]</td>\n",
       "      <td>[right, one, eureka]</td>\n",
       "      <td>right one eureka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText        asin  \\\n",
       "0                             good product and price  B00002N62Y   \n",
       "1  These belts are $2+ retail and some retailers/...  B00002N62Y   \n",
       "2  These came in a 2 pk and are perfect fit for m...  B00002N62Y   \n",
       "3  So my super fancy vacuum cleaner, the one with...  B00002N62Y   \n",
       "4                           Right one for my eureka.  B00002N62Y   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                        [good, product, and, price]   \n",
       "1  [these, belts, are, retail, and, some, retaile...   \n",
       "2  [these, came, in, pk, and, are, perfect, fit, ...   \n",
       "3  [so, my, super, fancy, vacuum, cleaner, the, o...   \n",
       "4                      [right, one, for, my, eureka]   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0                        [good, product, and, price]   \n",
       "1  [these, belt, are, retail, and, some, retailer...   \n",
       "2  [these, came, in, pk, and, are, perfect, fit, ...   \n",
       "3  [so, my, super, fancy, vacuum, cleaner, the, o...   \n",
       "4                      [right, one, for, my, eureka]   \n",
       "\n",
       "                                      cleaned_lemmas  \\\n",
       "0                             [good, product, price]   \n",
       "1  [belt, retail, retailer, seller, use, generic,...   \n",
       "2  [came, pk, perfect, fit, good, old, vac, seem,...   \n",
       "3  [super, fancy, vacuum, cleaner, one, cold, fus...   \n",
       "4                               [right, one, eureka]   \n",
       "\n",
       "                                  cleaned_lemmas_str  \n",
       "0                                 good product price  \n",
       "1  belt retail retailer seller use generic non oe...  \n",
       "2  came pk perfect fit good old vac seem strong d...  \n",
       "3  super fancy vacuum cleaner one cold fusion rea...  \n",
       "4                                   right one eureka  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 54492.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made unique tokens set\n",
      "made tokens-lemmas dict\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>category_correct</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>asin</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>cleaned_lemmas</th>\n",
       "      <th>cleaned_lemmas_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>It was a time honored tradition among the earl...</td>\n",
       "      <td>[Kitchen &amp; Dining, Dining &amp; Entertaining, Dinn...</td>\n",
       "      <td>{Kitchen, Plates, Dinner Plates, Dining, Enter...</td>\n",
       "      <td>You Are Special Today Red Plate [With Red Pen]</td>\n",
       "      <td>Waechtersbach USA</td>\n",
       "      <td>0001487795</td>\n",
       "      <td>[it, was, time, honored, tradition, among, the...</td>\n",
       "      <td>[it, wa, time, honored, tradition, among, the,...</td>\n",
       "      <td>[wa, time, honored, tradition, among, early, a...</td>\n",
       "      <td>wa time honored tradition among early american...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                        description  \\\n",
       "0      0  It was a time honored tradition among the earl...   \n",
       "\n",
       "                                            category  \\\n",
       "0  [Kitchen & Dining, Dining & Entertaining, Dinn...   \n",
       "\n",
       "                                    category_correct  \\\n",
       "0  {Kitchen, Plates, Dinner Plates, Dining, Enter...   \n",
       "\n",
       "                                            title              brand  \\\n",
       "0  You Are Special Today Red Plate [With Red Pen]  Waechtersbach USA   \n",
       "\n",
       "         asin                                             tokens  \\\n",
       "0  0001487795  [it, was, time, honored, tradition, among, the...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [it, wa, time, honored, tradition, among, the,...   \n",
       "\n",
       "                                      cleaned_lemmas  \\\n",
       "0  [wa, time, honored, tradition, among, early, a...   \n",
       "\n",
       "                                  cleaned_lemmas_str  \n",
       "0  wa time honored tradition among early american...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = meta[meta['description'] != 'no description'].reset_index()\n",
    "meta['tokens'] = ['no text'] * meta.shape[0]\n",
    "for i, com in enumerate(meta['description']):\n",
    "    if isinstance(com, str):\n",
    "        meta['tokens'][i] = tokenize_line(com)\n",
    "unique_toks, t_l = lemmatize_df(meta, 'tokens')\n",
    "meta['lemmas'] = meta['tokens'].apply(lambda x: lemmatize_line(t_l, x))\n",
    "meta['cleaned_lemmas'] = meta['lemmas'].apply(clean_lemmas)\n",
    "df_orig = meta.copy()\n",
    "meta['cleaned_lemmas_str'] = meta['cleaned_lemmas'].apply(lambda x: ' '.join(x))\n",
    "meta = meta.drop_duplicates('cleaned_lemmas_str')\n",
    "meta.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0)  You Are Special Today Red Plate [With Red Pen] \n",
      " Kitchen & Dining, Dining & Entertaining, Dinnerware, Plates, Dinner Plates \n",
      " wa time honored tradition among early american family someone deserved special praise attention served dinner red plate observe special occasion \n",
      "\n",
      "\n",
      "1)  Vicks Inhaler Relief for Cold Sinus Nasal Congestion Allergy \n",
      " Home Dcor, Candles & Holders, Candles \n",
      " vicks inhaler relief stuffy nose help sinus congestion breathe easy great allergy season made india \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(meta['cleaned_lemmas_str']):\n",
    "    if  i < 2:\n",
    "        print(f'{i}) ', meta['title'][i], '\\n', ', '.join(meta['category'][i]), '\\n', t, '\\n\\n')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пособираем самые частотные существительные корпуса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_counter(lines_list):\n",
    "    counter = []\n",
    "    progress_bar = tqdm(total=len(lines_list))\n",
    "    for line in lines_list:\n",
    "        line = nlp(line)\n",
    "        to_add = [t.text for t in line if t.text not in stopwords_list and t.pos_ == 'NOUN']\n",
    "        counter.extend(to_add)\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    return Counter(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nouns_counter(df.cleaned_lemmas_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vacuum', 12543),\n",
       " ('dyson', 3113),\n",
       " ('year', 3003),\n",
       " ('suction', 2911),\n",
       " ('floor', 2903),\n",
       " ('carpet', 2831),\n",
       " ('time', 2759),\n",
       " ('hose', 2618),\n",
       " ('bag', 2325),\n",
       " ('use', 2134)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, есть тут то, что нам нужно, это хорошо, но слова 'use', 'year', 'suction и 'time' кажутся тут лишними. Значит, попробуем сделать то же самое со словами из метадаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c2 = nouns_counter(meta.cleaned_lemmas_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inch', 8096),\n",
       " ('steel', 6228),\n",
       " ('knife', 4632),\n",
       " ('pan', 4224),\n",
       " ('piece', 3698),\n",
       " ('food', 3293),\n",
       " ('handle', 2976),\n",
       " ('hand', 2901),\n",
       " ('heat', 2882),\n",
       " ('feature', 2811)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут тоже встречается много \"мусора\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_preproc(list_line):\n",
    "    ans = []\n",
    "    for line in list_line:\n",
    "        line = re.sub(' &', '', line)\n",
    "        line = lemmatizer.lemmatize(line)\n",
    "        add = [w.lower() for w in line.split() if w not in stopwords_list]\n",
    "        ans.extend(add)\n",
    "    return ans\n",
    "\n",
    "cat_count = []\n",
    "for c in meta.category:\n",
    "     cat_count.extend(c)\n",
    "cat_counter = Counter(cat_count)\n",
    "cat_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_counter_nouns = nouns_counter(list(cat_counter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sets', 33),\n",
       " ('Knives', 26),\n",
       " ('Accessories', 12),\n",
       " ('Glasses', 11),\n",
       " ('Dishes', 8),\n",
       " ('Bags', 7),\n",
       " ('Bowls', 6),\n",
       " ('Covers', 6),\n",
       " ('Ovens', 6),\n",
       " ('Candy', 5)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_counter_nouns.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Урааа, вот это то, что нужно!!! Надо было только лемматизировать, сейчас поправим)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kitchen', 8474),\n",
       " ('dining', 8338),\n",
       " ('utensils', 1739),\n",
       " ('accessories', 1555),\n",
       " ('gadgets', 1501),\n",
       " ('entertaining', 1422),\n",
       " ('cookware', 1235),\n",
       " ('tools', 1055),\n",
       " ('sets', 1037),\n",
       " ('knife', 962)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_count_prec = cat_preproc(cat_count)\n",
    "cat_counter_prec = Counter(cat_count_prec)\n",
    "cat_counter_prec.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так точно лучше, и счет немного другой, оставляю даже без шлифовки существительных\n",
    "\n",
    "Еще с TFidf посчитаем частотные биграммы, и будет красота!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df=0.75, stop_words=stopwords_list, max_features=1000, ngram_range=(2,2))\n",
    "word_count_vector = cv.fit_transform(df.cleaned_lemmas_str.to_list())\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topn(feats, sorted_items, threshold=0.1):    \n",
    "    sorted_items = [item for item in sorted_items if item[1] > threshold]  \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feats[idx]\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feats[idx])\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results\n",
    "\n",
    "def sort_coo_matrix(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def important_features(text, threshold=0.1):\n",
    "    feats = cv.get_feature_names()\n",
    "    tf_idf_vector = tfidf_transformer.transform(cv.transform([text]))\n",
    "    sorted_items = sort_coo_matrix(tf_idf_vector.tocoo())\n",
    "    keywords = topn(feats, sorted_items, threshold)\n",
    "    return set(keywords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area rug',\n",
       " 'bagless upright',\n",
       " 'black decker',\n",
       " 'canister vacuum',\n",
       " 'carpet cleaner',\n",
       " 'dirt devil',\n",
       " 'easy clean',\n",
       " 'five star',\n",
       " 'hand held',\n",
       " 'hand vac',\n",
       " 'handheld vacuum',\n",
       " 'heavy duty',\n",
       " 'hepa filter',\n",
       " 'type vacuum',\n",
       " 'upright vacuum',\n",
       " 'vacuum cleaner',\n",
       " 'whole house'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_feats = important_features(' '.join(meta.title.to_list()), threshold=0.05)\n",
    "imp_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Огонь!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kitchen',\n",
       " 'dining',\n",
       " 'entertaining',\n",
       " 'dinnerware',\n",
       " 'plates',\n",
       " 'home',\n",
       " 'dcor',\n",
       " 'candles',\n",
       " 'holders',\n",
       " 'glassware']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_titles = [key for key,value in cat_counter_prec.items() if value > np.mean(list(cat_counter_prec.values()))]\n",
    "all_titles.extend(imp_feats)\n",
    "all_titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ngrams(' '.join(df.cleaned_lemmas_str).split(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('vacuum', 'cleaner'), 1346),\n",
       " (('work', 'great'), 706),\n",
       " (('work', 'well'), 611),\n",
       " (('pet', 'hair'), 501),\n",
       " (('dirt', 'devil'), 428),\n",
       " (('easy', 'use'), 407),\n",
       " (('light', 'weight'), 373),\n",
       " (('bare', 'floor'), 362),\n",
       " (('hardwood', 'floor'), 342),\n",
       " (('dog', 'hair'), 321)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_counter = Counter(bigrams)\n",
    "bigrams_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_filtered = []\n",
    "for b in bigrams_counter.keys():\n",
    "    if b[0] in all_titles or b[1] in all_titles:\n",
    "        bigrams_filtered.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fancy', 'vacuum')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_filtered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# # Экстрактор коллокаций\n",
    "finder2 = BigramCollocationFinder.from_documents(df.cleaned_lemmas)\n",
    "finder2.apply_freq_filter(5)\n",
    "#finder2.score_ngrams(bigram_measures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['permitted kitchen',\n",
       " 'pasta kitchen',\n",
       " 'random kitchen',\n",
       " 'kitchen counter',\n",
       " 'kitchen trashcan',\n",
       " 'tiled kitchen',\n",
       " 'minus kitchen',\n",
       " 'kitchen cabinet']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi = []\n",
    "for t in all_titles:\n",
    "    rate = [i for i in finder2.nbest(bigram_measures.pmi, 10000) if t in i]\n",
    "    pmi.append([' '.join(i) for i in rate])\n",
    "pmi[0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kitchen cabinet',\n",
       " 'kitchen bathroom',\n",
       " 'tiled kitchen',\n",
       " 'kitchen counter',\n",
       " 'kitchen trashcan',\n",
       " 'bathroom kitchen',\n",
       " 'please kitchen',\n",
       " 'kitchen tiled']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard = []\n",
    "for t in all_titles:\n",
    "    rate = [i for i in finder2.nbest(bigram_measures.jaccard, 10000) if t in i]\n",
    "    jaccard.append([' '.join(i) for i in rate])\n",
    "jaccard[0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kitchen cabinet',\n",
       " 'kitchen bathroom',\n",
       " 'tiled kitchen',\n",
       " 'kitchen counter',\n",
       " 'kitchen trashcan',\n",
       " 'bathroom kitchen',\n",
       " 'please kitchen',\n",
       " 'kitchen tiled']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice = []\n",
    "for t in all_titles:\n",
    "    rate = [i for i in finder2.nbest(bigram_measures.dice, 10000) if t in i]\n",
    "    dice.append([' '.join(i) for i in rate])\n",
    "dice[0][:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более-менее похожие выдачи с метрик, оставим  pmi, мне она понравилась больше всего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kitchen cabinet',\n",
       " 'kitchen bathroom',\n",
       " 'kitchen floor',\n",
       " 'tiled kitchen',\n",
       " 'kitchen counter',\n",
       " 'kitchen living',\n",
       " 'room kitchen',\n",
       " 'kitchen trashcan']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_ratio = []\n",
    "for t in all_titles:\n",
    "    rate = [i for i in finder2.nbest(bigram_measures.likelihood_ratio, 10000) if t in i]\n",
    "    likelihood_ratio.append([' '.join(i) for i in rate])\n",
    "likelihood_ratio[0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "for idx, item in enumerate(all_titles[:5]):\n",
    "    ans[item] = jaccard[idx][:5] + likelihood_ratio[idx][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kitchen cabinet', 'kitchen bathroom', 'tiled kitchen', 'kitchen counter', 'kitchen trashcan', 'kitchen cabinet', 'kitchen bathroom', 'kitchen floor', 'tiled kitchen', 'kitchen counter']\n"
     ]
    }
   ],
   "source": [
    "ans['kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den dining', 'dining room', 'dining table', 'bathroom dining', 'dining chair', 'dining room', 'room dining', 'den dining', 'living dining']\n"
     ]
    }
   ],
   "source": [
    "ans['dining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue plate', 'plate kitchen', 'dining plate', 'pocelain plate', 'wedding plate']\n"
     ]
    }
   ],
   "source": [
    "ans['plate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metal knife', 'bread knife', 'fish knife', 'steel knife', 'knife kitchen']\n"
     ]
    }
   ],
   "source": [
    "print(['metal knife', 'bread knife', 'fish knife', 'steel knife', 'knife kitchen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не могу сказать, что мне очень нравится то, что получилось. Есть, так сказать, к чему стремится.\n",
    "\n",
    "Я бы это связала и с самими отзывами: люди чаще описывают не сам товар, а как бы то, что связано с выбором этого товара: цена, качество, что тоже верно. Время использования или покупки, это полезная информация для дальнейшей автоматической оценки товара, даже для понимания периода его использования - christmas всякие штуки - зимой, мб какая-нибудь подставка под индейку - в июле и тд. Это и так очевидно жиому человеку, но не всегда машине. Или понимать, что скатерти нормально прослужить пару лет, а вот пылесос пусть бы работал более 10 лет.\n",
    "\n",
    "\n",
    "Как можно улучшить качество?\n",
    "\n",
    "Я бы точно посмотрела триграммы, как и хотела (но так расстроилась в биграммах, что забыла про это)\n",
    "\n",
    "Точно бы больше поработала с изначальной фильтрацией слов, которые мы считаем важными: может быть, что-то и ручками пописать и подополнять корпус важных слов с помощью word2vec, вообще поискала бы всякие NER-штуки в тексте и тоже бы их подектектила бы - это точно помогло бы нам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
