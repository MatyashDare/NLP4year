{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 1. Извлечение ключевых слов\n",
    "\n",
    "При выполнении домашнего задания можно пользоваться материалами лекций и семинаров.\n",
    "\n",
    "### Описание задания\n",
    "\n",
    "1. (1 балл) Подготовить мини-корпус (не меньше 4 текстов, примерный общий объём - 3-5 тысяч токенов) с разметкой ключевых слов. Предполагается, что вы найдете источник текстов, в котором уже выделены ключевые слова. Укажите источник корпуса и опишите, в каком виде там были представлены ключевые слова.\n",
    "\n",
    "2. (2 балла) Разметить ключевые слова самостоятельно. Оценить пересечение с имеющейся разметкой. Составить эталон разметки (например, пересечение или объединение вашей разметки и исходной).\n",
    "\n",
    "3. (2 балла) Применить к этому корпусу 3 метода извлечения ключевых слов на выбор (RAKE, TextRank, tf*idf, OKAPI BM25, ...)\n",
    "\n",
    "4. (2 балла) Составить морфологические/синтаксические шаблоны для ключевых слов и фраз, выделить соответствующие им подстроки из корпуса (например, именные группы Adj+Noun). Применить эти фильтры к спискам ключевых слов.\n",
    "\n",
    "5. (2 балла) Оценить точность, полноту, F-меру выбранных методов относительно эталона: с учётом морфосинтаксических шаблонов и без них.\n",
    "\n",
    "6. (1 балл) Описать ошибки автоматического выделения ключевых слов (что выделяется лишнее, что не выделяется); предложить свои методы решения этих проблем.\n",
    "\n",
    "###  Критерии оценки\n",
    "\n",
    "В каждом пункте указано количество баллов.\n",
    "\n",
    "###  Формат сдачи задания\n",
    "\n",
    "Jupyter-notebook на гитхабе (запишите адрес своего репозитория [сюда](https://docs.google.com/forms/d/e/1FAIpQLSfvoxiOKm9jnHO6v_ivGOeA3TKBT7Hg7bQlHa56MuALeMIcvQ/viewform?usp=sf_link))\n",
    "\n",
    "### Дедлайн\n",
    "\n",
    "7 ноября 2021 23:59мск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.** \n",
    "Все тексты собранного мной корпуса были взяты с сайта Российского агентства международной информации «РИА Новости». Мне очень нравится рубрика \"Культура\", однако были взяты тексты из рубрики \"Спорт\" и \"В мире\". Ключевые слова были даны внизу страницы с новостью. **Важное замечание:**  на страничке \"РИА Новостей\" чаще это были все-таки теги, а не ключевые слова статьи, поэтому взолотой корпус большинство оригинальных тегов не попало!. На каждый \"тег\" к статье можно нажать и выбрать для прочтения еще статьи с таким же ключевым словом. Я не брала в текст корпуса строчки типа \"ООН, 6 ноя — РИА Новости.\" или \"МОСКВА, 6 ноя — РИА Новости.\", так как скорее путали местоположение (например, речь шла про событие в Лондоне, а сам текст написан в Москве), к тому же не нужно \"забивать эфирное время\" не имеющими ключевого значения в самом тексте статьи словами \"РИА Новости\" и автора статьи.\n",
    "\n",
    "###  **2.** \n",
    "Все тексты я внимательно читала и выписывала вначале те ключевые слова, которые считала действительно ключевыми для каждого текста. Мой bias заключался всегда в том, что я знала, из какой категории новостей читаю текст, и всегда первым ключевым словом у меня был раздел новостей - например, \"Культура\". Далее я писала \"вид\" искусства или спорта: например, \"Футбол\" или \"Кино\"; затем мысленно просила себя одним словом описать, о чем был текст - так, например, для статьи про \"Шрека\" я добавила ключевое слово \"Воспитание\". \n",
    "\n",
    "\n",
    "Если были какие-то знаковые мероприятия, например, Букеровская премия, я тоже это включала в собственную разметку, так как 1) это название действительно часто упоминалось в тексте конткретной статьи 2) по такому ключевому слову хотелось бы действительно найти данную статью (моей перепроверкой всегда был вопрос : \"Логично ли по такому ключевому слову будет искать эту статью?\"). \n",
    "\n",
    "Эталон разметки был составлен, возможно, не так, как написано в задании - я не делала автоматически set(my_tags).intersection(original_tags), например, а просто после своей личной разметки читала теги, уже проставленные в газете (к сожалению, не могу с уверенностью сказать - теги ставят модераторы/редакторы (люди) или автоматически) и составляла список ключевых слов, наиболее для меня логичный. Например, в [статье про ABBA](https://ria.ru/20211105/abba-1757417178.html) фигурировало имя Мерил Стрип, но лишь один раз и это действительно не было ключевым именем в статье, но в тегах новостей имя актрисы было вынесено в качество ключевого слова - мне показалось это несколько нелогичным, и в колонку golden_tags это не попало). \n",
    "\n",
    "Я рада, что не вначала читала теги \"РИА Новостей\", а потом писала свои, а делала наоборот, потому что это помогало больше думать мне самой о выборе тега. Также я, например, не выносила все имена группы ABBA, оставила только название группы, потому что статья была все равно скорее про достижение самой группы, а не каждого отдельного ее участника. Еще для меня ключевые слова \"Культура\" и \"Новости культуры\" - одно и то же, потому что и так рассказывается о культуре на новостном портале) Оставляла только ключевое слово \"Культура\". Но все же в golden_tags я не вносила ключевые слова/теги, которые не употреблялись в тексте, например, в golden_tags не попали теги \"Культура\" или \"Кино и сериалы\", если речь шла о фильме, я писала \"Фильм\" в золотой стандарт. Мне это показалось самым верным и точным.\n",
    "\n",
    "\n",
    "###  **3.** \n",
    "Теперь применим автоматические методы извлечения ключевых слов и проанализируем результаты.\n",
    "\n",
    "\n",
    "Для начала импортируем все нужные модули:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pikachu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/pikachu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pikachu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "import re, regex\n",
    "import RAKE\n",
    "from summa import keywords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from yargy import Parser, rule, and_, or_\n",
    "from yargy.pipelines import morph_pipeline\n",
    "from yargy.predicates import gram, dictionary\n",
    "from yargy.interpretation import fact\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words('russian')\n",
    "stopwords_list.append('это')\n",
    "stopwords_list.append('который')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>web_page</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>original_tags</th>\n",
       "      <th>my_tags</th>\n",
       "      <th>golden_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Футбольный агент заявил, что \"Спартак\" ищет но...</td>\n",
       "      <td>https://rsport.ria.ru/20211105/spartak-1757812...</td>\n",
       "      <td>Спорт</td>\n",
       "      <td>Футбол, Марко Траббуки, Руй Витория, Российска...</td>\n",
       "      <td>Спорт, Футбол, Спартак, новый тренер, Руй Вито...</td>\n",
       "      <td>Футбол, Спартак, новый тренер, Руй Витория, За...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Снова первые и лучшие. ABBA отправляется в бес...</td>\n",
       "      <td>https://ria.ru/20211105/abba-1757417178.html</td>\n",
       "      <td>Культура</td>\n",
       "      <td>Культура, Аманда Сейфрид, ABBA, Пирс Броснан, ...</td>\n",
       "      <td>Культура, Музыка, Поп, ABBA, Шоу-бизнес, Поп-м...</td>\n",
       "      <td>Музыка, ABBA, Технологии, Шоу-бизнес, Поп-музы...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Тролль, брекеты, подставной питон. Как снимали...</td>\n",
       "      <td>https://ria.ru/20211104/potter-1757573549.html</td>\n",
       "      <td>Культура</td>\n",
       "      <td>Культура, Стивен Спилберг, Великобритания, Джо...</td>\n",
       "      <td>Культура, Кино, Книги, Гарри Поттер, За кадром...</td>\n",
       "      <td>Фильм, Книги, Гарри Поттер</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Футбольный агент заявил, что \"Спартак\" ищет но...   \n",
       "1  Снова первые и лучшие. ABBA отправляется в бес...   \n",
       "2  Тролль, брекеты, подставной питон. Как снимали...   \n",
       "\n",
       "                                            web_page subcategory  \\\n",
       "0  https://rsport.ria.ru/20211105/spartak-1757812...       Спорт   \n",
       "1       https://ria.ru/20211105/abba-1757417178.html    Культура   \n",
       "2     https://ria.ru/20211104/potter-1757573549.html    Культура   \n",
       "\n",
       "                                       original_tags  \\\n",
       "0  Футбол, Марко Траббуки, Руй Витория, Российска...   \n",
       "1  Культура, Аманда Сейфрид, ABBA, Пирс Броснан, ...   \n",
       "2  Культура, Стивен Спилберг, Великобритания, Джо...   \n",
       "\n",
       "                                             my_tags  \\\n",
       "0  Спорт, Футбол, Спартак, новый тренер, Руй Вито...   \n",
       "1  Культура, Музыка, Поп, ABBA, Шоу-бизнес, Поп-м...   \n",
       "2  Культура, Кино, Книги, Гарри Поттер, За кадром...   \n",
       "\n",
       "                                         golden_tags  \n",
       "0  Футбол, Спартак, новый тренер, Руй Витория, За...  \n",
       "1  Музыка, ABBA, Технологии, Шоу-бизнес, Поп-музы...  \n",
       "2                         Фильм, Книги, Гарри Поттер  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('news_tags.csv', delimiter=';')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем препроцессинг так:\n",
    "\n",
    "1. Токенизация\n",
    "2. Токенизация + нормализация (убираем топ-n% самых частотных и самых редких слов)\n",
    "3. Токенизация + лемматизация\n",
    "4. Токенизация + лемматизация + удаление стоп-слов\n",
    "5. Токенизация + лемматизация + удаление стоп-слов + нормализация (убираем топ-n% самых частотных и самых редких слов)\n",
    "\n",
    "Буду брать везде по топ-10 ключевых слов, думаю, наши алгоритмы могли бы быть \"советчиками\" ключевых слов, а хорошо советовать на бОльшем числе слов)\n",
    "\n",
    "\n",
    "UPD: сравнивать будем только токенизированные и лемматизированные тексты, так как нормализация и удаление стоп-слов и так используется в алгоритмах ниже, я про это вспомнила уже после препоцессинга :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line: str) -> str:\n",
    "    to_lem = ' '.join(tokenizer.tokenize(line))\n",
    "    return ''.join(to_lem)\n",
    "\n",
    "def lemmatize(line: str) -> str:\n",
    "    words = [w.lower() for w in mystem_analyzer.lemmatize(line)]\n",
    "    return ''.join(words)\n",
    "\n",
    "def clean_stopwords(line: str) -> str:\n",
    "    return ' '.join([w for w in line.split() if w not in stopwords_list])\n",
    "\n",
    "def normalize(df, column_name, left=0.01, right=0.01):\n",
    "    all_words = []\n",
    "    for text in df[column_name]:\n",
    "        all_words.extend(text)\n",
    "    c = Counter(all_words)\n",
    "    to_del = c.most_common(int(len(c)*right))\n",
    "    to_del.extend(c.most_common()[-int(len(c)*left):])\n",
    "\n",
    "    cleared = []\n",
    "    for l in df[column_name]:\n",
    "        cleared.append(clear(l.split(), to_del))\n",
    "    df[f'cleared_{column_name}'] = cleared\n",
    "    return df[f'cleared_{column_name}']\n",
    "\n",
    "def clear(line: str, to_del) -> str:\n",
    "    return ' '.join([w for w in line if w not in to_del])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].apply(tokenize)\n",
    "df['cleared_tokens'] = normalize(df, 'tokens', left=0.02, right=0.03)\n",
    "df['lemmas'] = df['tokens'].apply(lemmatize)\n",
    "df['lemmas_no_stopwords'] = df['lemmas'].apply(clean_stopwords)\n",
    "df['cleared_lemmas_no_stopwords'] = normalize(df, 'lemmas_no_stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 1. RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rake_tags(texts, top_n):\n",
    "    tags = []\n",
    "    rake = RAKE.Rake(stopwords_list)\n",
    "    tag_tuples = rake.run(texts, maxWords=1, minFrequency=2)\n",
    "    for (tag, prob) in tag_tuples:\n",
    "        if prob > 0:\n",
    "            tags.append(tag)\n",
    "    # если нужного кол-ва top_n ключевых слов не набирается, чуть упрощаем поиск (и далее так же)\n",
    "    if len(tags) < top_n:\n",
    "        tag_tuples = rake.run(texts, maxWords=2, minFrequency=1)\n",
    "        for (tag, prob) in tag_tuples:\n",
    "            if prob > 2:\n",
    "                if len(tags) < top_n:\n",
    "                    tags.append(tag)\n",
    "    return ', '.join(tags[:top_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RAKE_tokens'] = df['tokens'].apply(lambda x: rake_tags(x, top_n=10))\n",
    "df['RAKE_lemmas'] = df['lemmas'].apply(lambda x: rake_tags(x, top_n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS:\n",
      "0.  спартак, замену витории, витория возглавил, уход витории, вопрос времени, такими результатами, новым тренером, моим данным, массимо каррерой, спортивного директора\n",
      "1.  voyage, молодыми, поп, бенни, студии, собственно, альбом, какими, возможно, don\n",
      "2.  кастинг, кстати, подставной питон, гарри поттере, философский камень, мог оказаться, актеры должны, мэгги смит, факультета слизерина, любви северуса\n",
      "3.  шрек, любовь, шрека, сша, огр, сказках, шреке, неудивительно, ждут принца, страшные чудища\n",
      "4.  крыму, произошло, оборонительных потребностей, которые проходили, квартире организации, словам белоусова, москву вновь, пытались обвинить, черного морей, военное присутствие\n",
      "5.  реальностью, реджани, съемках фильма, дом гуччи, психологических проблемах, процессе съемок, фильмом длилась, глубоко погрузилась, концу съемок, съемочной площадке\n",
      "6.  друзья, числе, друзей, американскому сериалу, которой расскажет, семизначную сумму, откровенная книга, время съемок, зависимости актера, отмечает издание\n",
      "7.  роман, обещание, премию, шорт, великобритании, ирландии, хороший доктор, странной комнате, белой семье, южной африке\n",
      "\n",
      " LEMMAS:\n",
      "0.  спартак, замена витория, витория возглавлять, уход витория, вопрос время, новый тренер, массимо каррера, спортивный директор, рассказывать трабукки, интервью championat\n",
      "1.  выходить, voyage, революция, молодой, поп, бенни, агнета, ради, сцена, студия\n",
      "2.  тролль, кастинг, кстати, сказать, сцена, место, подставной питон, философский камень, мочь оказываться, мэгги смит\n",
      "3.  шрек, принцесса, сказка, именно, замок, сша, вход, огр, неудивительный, бизнес\n",
      "4.  крым, превышать, государство, происходить, квартира организация, слово белоусова, москва вновь, черный море, военный присутствие, представлять угроза\n",
      "5.  реальность, жить, реджани, дом гучча, психологический проблема, процесс съемка, глубоко погружаться, конец съемка, съемочный площадка, италия прогуливаться\n",
      "6.  друг, число, семизначный сумма, откровенный книга, время съемка, зависимость актер, отмечать издание, главный роль, терять популярность, кортни кокс\n",
      "7.  роман, обещание, премия, шорты, великобритания, ирландия, хороший доктор, странный комната, белый семья, южный африка\n"
     ]
    }
   ],
   "source": [
    "print('TOKENS:')\n",
    "for i, t in enumerate(df['RAKE_tokens']):\n",
    "    print(f'{i}. ', t)\n",
    "print('\\n LEMMAS:')\n",
    "for i, t in enumerate(df['RAKE_lemmas']):\n",
    "    print(f'{i}. ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я специально вывожу, как работает алгоритм на токенах и на леммах (помним, что в обоих случаях убираются стоп-слова - тут сразу же N.B., что стоп-слова чаще употореблены в начальной форме, не все убрано из токенов, например; а также есть ограничения на число употреблений слов (если хватает для вывода  top-n слов).\n",
    "\n",
    "Мне иногда больше нравится то, что выводится на токенах, например \"Замену Витории\" звучит более \"по-журналистки\", чем просто \"Замена Витория\", также видны несовешенства автоматических методов обработки текстов, например, \"шорт-лист\" стало \"шорт\", \"лист\", а \"шорт\" стало \"шортами\"))))) \n",
    "\n",
    "Честно говоря, я бы золотой корпус составила, основываясь на выдаче автоматической, потому что заметила подходящие словосочетания и слова. Или же для сравнения я взяла бы что-то как микс (на мое усмотрение) списков ключевых слов из лемм или из токенов)\n",
    "\n",
    "**Анализ вывода:** (будет в основном про леммы, так как токены и по метрикам вдальнейшем \"проигрывают\", и в целом лучше по леммам смотреть ключевые слова на всех алгоритмах) \n",
    "\n",
    "ЛЕММЫ:\n",
    "\n",
    "есть совпадения с эталоном, есть просто статистически частые биграммы, например, \"спортивный директор\", хорошо, что и имена действительно ключевые в статьях прослеживались, НО нехорошо, что иногда брались имена \"как надо, как привычно\" -  конструкцией \"Имя Фамилия\", а иногда просто именами, например, просто \"Бенни\", что само по себе не так плохо, но и нелогично все же. В некоторых случаях были и бессмысленные с точки зрения информативности/ нужности такого кандидата в качестве ключевого слова словосочетания выделены, например\" \"мочь казаться\" или \"Москва вновь\", но я, например, по некоторым предсказанным ключевым словам действительно бы хотела пересмотреть свою \"золотую\" ращметку и добавить несколько предложенных, например, для статьи про первые съёмки Гарри Поттера правда логично добавить \"Филосософский камень\", \"Кастинг\" или про новую книги - лауретара премии добавить \"окткровенная книга\", а словосочетания \"психологическая проблема\" и \"процесс съёмки\" - в статью про съёемки фильма \"Дом Гуччи\". \n",
    "\n",
    "ТОКЕНЫ:\n",
    "\n",
    "Как было ранее отмечено, леммы точнее выделяют ключевые слова, но иногда более точная \"стилистическая окраска\" есть и ключевых слов на токенах. Повторюсь, что классно было бы \"миксовать\" эти два подхода, но как потом автоматически отбирать, что лучше, пока непонятно.\n",
    "\n",
    "Общий итог:\n",
    "\n",
    "я бы поубирала изначально в тексте всякие дежурные фразы из разряда: \"заявил продюссер\", \"поделился с нами актер\", в общем, все глагольные и именные группы, которые вводят прямую речь или в целом имеют значение процесса говорения, а также разные линкеры типа \"кстати\", \"однако\", \"к тому же\", с самим алгоритмом особо не \"поборешься\", поэтому стоит больше времени уделить препроцессингу или, вернее, постпроцессингу после первых выделенных ключевых слов повыкидывать еще какие-то паттерны и заново запустить алгоритм.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_tags(text, top_n):\n",
    "    tag_list = keywords.keywords(text, language='russian', \n",
    "                           additional_stopwords=stopwords_list)\n",
    "    return ', '.join(tag_list.split('\\n')[:top_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TextRank_tokens'] = df['tokens'].apply(lambda x: textrank_tags(x, top_n=10))\n",
    "df['TextRank_lemmas'] = df['lemmas'].apply(lambda x: textrank_tags(x, top_n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS:\n",
      "0.  спартак, спартака, агент, агенты, тренера, тренеру, новым тренером, нового, витории, витория\n",
      "1.  abba, группой, шоу, сама группа, самые, самых, сами, бьорн, бьорна, новый альбом группы\n",
      "2.  поттере, поттер, поттера, актеры, актеру, актер, актерам, артист, артисты, артиста\n",
      "3.  шрек, шрека, шреке, шреком, принцессы, принцессах, принцесс, принцессу, принцесса, принца\n",
      "4.  которые, которая, котором, российской, российским, российского, крыму, крым, крыма, военное\n",
      "5.  гага, гаги, гагой, фильма, фильмом, фильме, года, годам, месяцев, итальянского\n",
      "6.  друзья, друзей, сериал, актер, актера, звезда сериала, перри, книга, исполнилось, исполнили\n",
      "7.  премию, года, году, год, годы, букеровской премии, букеровская премия, обещание, английской, английском\n",
      "\n",
      " LEMMAS:\n",
      "0.  спартак, агент, тренер, витория, вопрос, свой, искать новый, трабукки, лига, таблица\n",
      "1.  abba, шоу, бьорн, музыкальный, новый альбом группа, весь, песня, выходить, выход, сделать\n",
      "2.  артист, гарри поттер, весь актер, сцена, кстати, гермиона, главный, волшебный, книга, поэтому\n",
      "3.  шрек, принцесса, сказка, новый, принц, зритель, мультфильм становиться, фаркуад, i, сказочный канон\n",
      "4.  российский, крым, военный, год, оон, россия, свой, дипломат белоусов, защита наш, украина\n",
      "5.  год, леди гага, съемка фильм, итальянский, реджани, маурицио, поведение, месяц, женщина, киллер\n",
      "6.  друг, звезда сериал, перри, актер мэттью, автобиографический книга, мэтт, выходить, год, исполняться, исполнять\n",
      "7.  год, обещание, лауреат букеровский премия, становиться, английский, роман, автор, писатель, британский, престижный\n"
     ]
    }
   ],
   "source": [
    "print('TOKENS:')\n",
    "for i, t in enumerate(df['TextRank_tokens']):\n",
    "    print(f'{i}. ', t)\n",
    "print('\\n LEMMAS:')    \n",
    "for i, t in enumerate(df['TextRank_lemmas']):\n",
    "    print(f'{i}. ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В токенах как-то уж очень много разновидностей словоформ, но, надо сказать, что леммы этих слов действительно не такие уж и \"неключвые\" слова\n",
    "\n",
    "В этом алгоритме скорее однословные ключевые слова выделены, тут даже есть упор на просто прилагательных каких-то достаточно тоже важных в тексте, но идеально было бы тогда \"подтягивать\" к такому прилагательному и имя, к которому оно относится - было бы красиво!\n",
    "\n",
    "есть, конечно, и \"ерунда\" с точки зрения человеческой оценки, например \"искать новый\" - тут опять же лучше с именем существительным сделать \"искать нового тренера\", есть и просто частотные подлежащее-сказуемое пары, например, \"мультфильм становится\" и вообще надо заметить, что слова \"становиться\", \"исполнять\", \"сделать\", в общем, глаголы/существительные, описывающие развитие/становление процесса выделяются алгоритмом. Мне кажется, это здорово с точки зрения семантики, но иногда это не очень релевантно общему посылку текста.\n",
    "\n",
    "мне не нравится, что имена опять-таки, как и в первом алгоритме, часто выделяются не вместе: просто имя/ просто фамилия, но не в сочетании \"имя-фамилия\" - я все еще за вариант выделять с помощью NER-парсера именованные сущности и по статистической значимости выдавать самые частотные относительно определенного порога или относительно частотности употреблений всех NER'ов в тексте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Tfidf-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_tags(text, top_n):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords_list, \n",
    "                             ngram_range=(1,2))\n",
    "    tfidf = vectorizer.fit_transform([text])\n",
    "    weights = np.argsort(np.asarray(tfidf.sum(axis=0)).ravel())[::-1]\n",
    "    tfidf_feature_names = np.array(vectorizer.get_feature_names())\n",
    "    return ', '.join(tfidf_feature_names[weights[:top_n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tfidf_tokens'] = df['tokens'].apply(lambda x: tfidf_tags(x, top_n=10))\n",
    "df['Tfidf_lemmas'] = df['lemmas'].apply(lambda x: tfidf_tags(x, top_n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS:\n",
      "0.  спартак, ищет, спартак ищет, замену, витории, тренера, агент, нового, главному тренеру, ищет замену\n",
      "1.  abba, шоу, группы, бенни, поп, вместе, четверка, снова, бьорн, voyage\n",
      "2.  гарри, поттера, кстати, поэтому, постоянно, итоге, факультета, фильм, должен, рэдклифф\n",
      "3.  шрек, огр, каноны, обычно, сказка, шреке, именно, любовь, шрека, шоу\n",
      "4.  крыму, россии, деятельности, присутствие, обвинения, военное, военное присутствие, военной, военной деятельности, морей\n",
      "5.  гуччи, реджани, леди, дом гуччи, дом, гага, фильма, актриса, леди гага, связь реальностью\n",
      "6.  друзья, перри, сериала, сериала друзья, издание, перри заключил, друзья перри, мэттью, мэттью перри, звезда\n",
      "7.  года, премии, роман, великобритании, премию, 2021, обещание, году, ирландии, букеровской премии\n",
      "\n",
      " LEMMAS:\n",
      "0.  спартак, тренер, искать, главный тренер, агент, главный, спартак искать, витория, новый, замена\n",
      "1.  abba, группа, шоу, новый, бенни, первый, бьорн, песня, поп, выходить\n",
      "2.  гарри, поттер, актер, снимать, сцена, мочь, артист, весь, книга, роль\n",
      "3.  шрек, сказка, принцесса, принц, мультфильм, герой, огр, новый, канон, зритель\n",
      "4.  крым, военный, россия, деятельность, российский, военный деятельность, море, военный присутствие, акватория, чрезмерный военный\n",
      "5.  фильм, гучча, гага, год, съемка, леди, реджани, дом гучча, дом, леди гага\n",
      "6.  друг, сериал, перри, год, сериал друг, книга, звезда, автобиографический, автобиографический книга, актер\n",
      "7.  год, премия, роман, обещание, великобритания, становиться, букеровский премия, букеровский, лауреат, 2021\n"
     ]
    }
   ],
   "source": [
    "print('TOKENS:')\n",
    "for i, t in enumerate(df['Tfidf_tokens']):\n",
    "    print(f'{i}. ', t)\n",
    "print('\\n LEMMAS:')    \n",
    "for i, t in enumerate(df['Tfidf_lemmas']):\n",
    "    print(f'{i}. ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "фамилии-имена иногда разделяются, иногда сохраняются (здорово, что \"леди гагу\" оставили, как, впрочем, и  TextRank с этим справился, а вот RAKE - нет)\n",
    "\n",
    "\"тренер\", \"главный тренер\", \"новый\" - вот жалко, что биграмма \"новый тренер\" не была выдана, потому что \"тренер\" и \"главный тренер\" особо не так важны в тексте статьи, как факт поиска нового тренера для ФК\n",
    "\n",
    "опять выделяются в качестве ключевых всякие линкеры иногда, например, \"поэтому\" - я в стоп-слова добавляла только \"это\" и \"который\", вот стоило бы добавить все линкеры тоже\n",
    "\n",
    "первый алгоритм, выделивший год книжной премии 2021 - это важно, но я бы это не отдельным ключевым словом, наверное, выводила бы (хотя тоже неплохо!), а, например, \"букеровская премия - 2021\", но, так сказать, многого хочу, тут и так опять прилагательные (я бы сказала, действительно важные, например, выделенные прилагательные \"букеровский\" или \"автобиографический\", но я бы как-то подищала потом правилами, что раз TFidf такой умница и выделил \"букеровский премия\", \"военный деятельность\", \"военный присутствие\", \"автобиографический книга\" - все действительно важные словосочетания(!!!!), то тогда отдельно не выводить прилагательные\n",
    "\n",
    "в общем, как-то \"доводить до ума\" еще правилами выдачу, брать, например, топ-50 слов, если выводится не биграмма (биграммы, например, сразу берем в ответ), то не брать в ответ униграммы, входящие в состави биграмм, а еще смотреть эти униграммы на попарную сочетаемость - если они часто друг с другом стоят в тексте - можно их соединить в одно ключевое словосочетание, например, тоже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['golden_tags_tokens'] = df['golden_tags'].apply(tokenize)\n",
    "df['golden_tags_lemmas'] = df['golden_tags'].apply(lemmatize).apply(lambda x: re.sub('\\n', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>golden_tags_lemmas</th>\n",
       "      <th>RAKE_lemmas</th>\n",
       "      <th>TextRank_lemmas</th>\n",
       "      <th>Tfidf_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>футбол, спартак, новый тренер, руй витория, за...</td>\n",
       "      <td>спартак, замена витория, витория возглавлять, ...</td>\n",
       "      <td>спартак, агент, тренер, витория, вопрос, свой,...</td>\n",
       "      <td>спартак, тренер, искать, главный тренер, агент...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>музыка, abba, технология, шоу-бизнес, поп-музы...</td>\n",
       "      <td>выходить, voyage, революция, молодой, поп, бен...</td>\n",
       "      <td>abba, шоу, бьорн, музыкальный, новый альбом гр...</td>\n",
       "      <td>abba, группа, шоу, новый, бенни, первый, бьорн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>фильм, книга, гарри поттер</td>\n",
       "      <td>тролль, кастинг, кстати, сказать, сцена, место...</td>\n",
       "      <td>артист, гарри поттер, весь актер, сцена, кстат...</td>\n",
       "      <td>гарри, поттер, актер, снимать, сцена, мочь, ар...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>кино, шрек, мультфильм, ценность</td>\n",
       "      <td>шрек, принцесса, сказка, именно, замок, сша, в...</td>\n",
       "      <td>шрек, принцесса, сказка, новый, принц, зритель...</td>\n",
       "      <td>шрек, сказка, принцесса, принц, мультфильм, ге...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>украина, крым, мид</td>\n",
       "      <td>крым, превышать, государство, происходить, ква...</td>\n",
       "      <td>российский, крым, военный, год, оон, россия, с...</td>\n",
       "      <td>крым, военный, россия, деятельность, российски...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>фильм, леди гага, «дом гучча», премьера, гучча</td>\n",
       "      <td>реальность, жить, реджани, дом гучча, психолог...</td>\n",
       "      <td>год, леди гага, съемка фильм, итальянский, ред...</td>\n",
       "      <td>фильм, гучча, гага, год, съемка, леди, реджани...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>сериал, сериал «друг»,  мэттью перри, друг</td>\n",
       "      <td>друг, число, семизначный сумма, откровенный кн...</td>\n",
       "      <td>друг, звезда сериал, перри, актер мэттью, авто...</td>\n",
       "      <td>друг, сериал, перри, год, сериал друг, книга, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>книга, букеровский премия, дэймон гулгут</td>\n",
       "      <td>роман, обещание, премия, шорты, великобритания...</td>\n",
       "      <td>год, обещание, лауреат букеровский премия, ста...</td>\n",
       "      <td>год, премия, роман, обещание, великобритания, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  golden_tags_lemmas  \\\n",
       "0  футбол, спартак, новый тренер, руй витория, за...   \n",
       "1  музыка, abba, технология, шоу-бизнес, поп-музы...   \n",
       "2                         фильм, книга, гарри поттер   \n",
       "3                   кино, шрек, мультфильм, ценность   \n",
       "4                                 украина, крым, мид   \n",
       "5     фильм, леди гага, «дом гучча», премьера, гучча   \n",
       "6         сериал, сериал «друг»,  мэттью перри, друг   \n",
       "7           книга, букеровский премия, дэймон гулгут   \n",
       "\n",
       "                                         RAKE_lemmas  \\\n",
       "0  спартак, замена витория, витория возглавлять, ...   \n",
       "1  выходить, voyage, революция, молодой, поп, бен...   \n",
       "2  тролль, кастинг, кстати, сказать, сцена, место...   \n",
       "3  шрек, принцесса, сказка, именно, замок, сша, в...   \n",
       "4  крым, превышать, государство, происходить, ква...   \n",
       "5  реальность, жить, реджани, дом гучча, психолог...   \n",
       "6  друг, число, семизначный сумма, откровенный кн...   \n",
       "7  роман, обещание, премия, шорты, великобритания...   \n",
       "\n",
       "                                     TextRank_lemmas  \\\n",
       "0  спартак, агент, тренер, витория, вопрос, свой,...   \n",
       "1  abba, шоу, бьорн, музыкальный, новый альбом гр...   \n",
       "2  артист, гарри поттер, весь актер, сцена, кстат...   \n",
       "3  шрек, принцесса, сказка, новый, принц, зритель...   \n",
       "4  российский, крым, военный, год, оон, россия, с...   \n",
       "5  год, леди гага, съемка фильм, итальянский, ред...   \n",
       "6  друг, звезда сериал, перри, актер мэттью, авто...   \n",
       "7  год, обещание, лауреат букеровский премия, ста...   \n",
       "\n",
       "                                        Tfidf_lemmas  \n",
       "0  спартак, тренер, искать, главный тренер, агент...  \n",
       "1  abba, группа, шоу, новый, бенни, первый, бьорн...  \n",
       "2  гарри, поттер, актер, снимать, сцена, мочь, ар...  \n",
       "3  шрек, сказка, принцесса, принц, мультфильм, ге...  \n",
       "4  крым, военный, россия, деятельность, российски...  \n",
       "5  фильм, гучча, гага, год, съемка, леди, реджани...  \n",
       "6  друг, сериал, перри, год, сериал друг, книга, ...  \n",
       "7  год, премия, роман, обещание, великобритания, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['golden_tags_lemmas', 'RAKE_lemmas', 'TextRank_lemmas', 'Tfidf_lemmas']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Морфо-синтаксические правила"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я посмотрела глазами (и перепроверила кодом (не сохранила, тк не посчитала нужным сохранять автоматическую проверку)), что чаще всего ключевые слова - именные группы (просто существительное, 2 существительных, существительное + прилагательное или какое-то имя и фамилия. Для нахождения нужных мне паттернов пользовалась [документацией yargy-парсера](https://github.com/natasha/yargy). Получилось здорово)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = rule(gram('NOUN'))\n",
    "NP = rule(gram('NOUN'), gram('NOUN'))\n",
    "AdjP = rule(gram('ADJF'), gram('NOUN'))\n",
    "\n",
    "Name = fact(\n",
    "    'Name',\n",
    "    ['first', 'last']\n",
    ")\n",
    "\n",
    "\n",
    "NAME = rule(\n",
    "    gram('Name').interpretation(\n",
    "        Name.first.inflected()\n",
    "    ),\n",
    "    gram('Surn').interpretation(\n",
    "        Name.last.inflected()\n",
    "    )\n",
    ").interpretation(\n",
    "    Name\n",
    ")\n",
    "\n",
    "\n",
    "grammar = Parser(\n",
    "    or_(\n",
    "        NAME,\n",
    "        N,\n",
    "        NP,\n",
    "        AdjP\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rules(tags):\n",
    "    ans_tags = []\n",
    "    for tag in tags.split(', '):\n",
    "        for match in grammar.findall(tag):\n",
    "            ans_tags.append(' '.join([_.value for _ in match.tokens]))\n",
    "    return ', '.join(ans_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RAKE_lemmas_ruled'] = df['RAKE_lemmas'].apply(find_rules)\n",
    "df['TextRank_lemmas_ruled'] = df['TextRank_lemmas'].apply(find_rules)\n",
    "df['Tfidf_lemmas_ruled'] = df['Tfidf_lemmas'].apply(find_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "По итогу не стало суть меньше тегов для RAKE на 3, TextRank - на 0, Tfidf - на 1.\n"
     ]
    }
   ],
   "source": [
    "r = df[df['RAKE_lemmas_ruled'].apply(lambda x: len(x.split(', '))) == df['RAKE_lemmas'].apply(lambda x: len(x.split(', ')))].shape[0]\n",
    "tr = df[df['TextRank_lemmas_ruled'].apply(lambda x: len(x.split(', '))) == df['TextRank_lemmas'].apply(lambda x: len(x.split(', ')))].shape[0]\n",
    "tf = df[df['Tfidf_lemmas_ruled'].apply(lambda x: len(x.split(', '))) == df['Tfidf_lemmas'].apply(lambda x: len(x.split(', ')))].shape[0]\n",
    "print(f'По итогу не стало суть меньше тегов для RAKE на {r}, TextRank - на {tr}, Tfidf - на {tf}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Подсчет метрик: точность, полнота, F-мера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала напишем функции для подсчета всех метрик: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_line(golden_tags, tags):\n",
    "    golden_tags = set(golden_tags.split(', '))\n",
    "    tags = set(tags.split(', '))\n",
    "    return len(tags.intersection(golden_tags))/len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_line(golden_tags, tags):\n",
    "    golden_tags = set(golden_tags.split(', '))\n",
    "    tags = set(tags.split(', '))\n",
    "    return len(tags.intersection(golden_tags))/len(golden_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_score_line(precision, recall):\n",
    "    return 2 * precision * recall / (precision + recall + 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_metrics(golden_tags, tags):\n",
    "    precision = precision_line(golden_tags, tags)\n",
    "    recall = recall_line(golden_tags, tags)\n",
    "    F_score = round(F_score_line(precision, recall), 2)\n",
    "    return [round(precision, 2), round(recall, 2), F_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем метрики на леммах (без применения морфо-синтаксических правил отбора ключевых слов): \n",
    "\n",
    "P.S.: на токенах практически в 2 раза хуже сработали все показатели (что и логично, исходя из способа оценивания точности и полноты), я не включила в итоговый отчет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_metrics = []\n",
    "textrank_metrics = []\n",
    "tfidf_metrics = []\n",
    "for i in range(df.shape[0]):\n",
    "    rake_metrics.append(count_metrics(df['golden_tags_lemmas'][i], df['RAKE_lemmas'][i]))\n",
    "    textrank_metrics.append(count_metrics(df['golden_tags_lemmas'][i], df['TextRank_lemmas'][i])) \n",
    "    tfidf_metrics.append(count_metrics(df['golden_tags_lemmas'][i], df['Tfidf_lemmas'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(list_metrics):\n",
    "    array_metrics = np.array(list_metrics)\n",
    "    mean_precision = round(array_metrics[:,0:1].mean(), 2)\n",
    "    mean_recall = round(array_metrics[:,1:2].mean(), 2)\n",
    "    mean_f_score = round(array_metrics[:,2:3].mean(), 2)\n",
    "    return [mean_precision, mean_recall, mean_f_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: recall: F-score:\n",
      "0.09\t0.2\t0.12\n",
      "0.12\t0.32\t0.17\n",
      "0.18\t0.4\t0.24\n"
     ]
    }
   ],
   "source": [
    "print('precision: recall: F-score:')\n",
    "print('\\t'.join(map(str,print_metrics(rake_metrics))))\n",
    "print('\\t'.join(map(str,print_metrics(textrank_metrics))))\n",
    "print('\\t'.join(map(str,print_metrics(tfidf_metrics))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но для отоборанных правилами ключевых слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_metrics2 = []\n",
    "textrank_metrics2 = []\n",
    "tfidf_metrics2 = []\n",
    "for i in range(df.shape[0]):\n",
    "    rake_metrics2.append(count_metrics(df['golden_tags_lemmas'][i], df['RAKE_lemmas_ruled'][i]))\n",
    "    textrank_metrics2.append(count_metrics(df['golden_tags_lemmas'][i], df['TextRank_lemmas_ruled'][i])) \n",
    "    tfidf_metrics2.append(count_metrics(df['golden_tags_lemmas'][i], df['Tfidf_lemmas_ruled'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: recall: F-score:\n",
      "0.1\t0.2\t0.13\n",
      "0.17\t0.33\t0.22\n",
      "0.2\t0.38\t0.26\n"
     ]
    }
   ],
   "source": [
    "print('precision: recall: F-score:')\n",
    "print('\\t'.join(map(str,print_metrics(rake_metrics2))))\n",
    "print('\\t'.join(map(str,print_metrics(textrank_metrics2))))\n",
    "print('\\t'.join(map(str,print_metrics(tfidf_metrics2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что на отобранных правилами ключевых словах на всех трех метриках увеличилась точность, но при этом для RAKE полнота осталась такой же, для TextRank - на сотую увеличилась, а для TFidf - на 2 сотых стала меньше. Это говорит о том, что мы лучше отобрали ключевые слова из того, что модели предсказали, (то есть из того, что было предсказано, мы правилами отобрали то, что по итогу в совпадало на эталонные ключевые слова), но просто сами по себе изначальные предсказания были не самыми лучшими. \n",
    "\n",
    "Итог: мы с правилами стали лучше отбирать из top-n предсказаний модели \"более похожие на эталон\" ключевые слова, но все еще стоит лучше предсказывать сами эти ключевые слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-первых, я бы сделала \"помягче\" метрики: считаю, что если модель выделила \"Друзья\", а не \"сериал \"Друзья\"\" это, например, 0,5 балла, а не 0. То есть мне хочется думать про \"разбалловку\" еще самих ключевых слов, а не проверять точное вхождение того или иного ключевого слова или словосочетания. Возможно, даже смотреть на вхождение какой-то леммы из эталона в предсказанных ключевых словах.\n",
    "\n",
    "Во-вторых, хочется в препроцессинге не лемматизировать и никак не изменять особо все NER-ы, потому что для той же статьи, связанной с одним из актеров сериала/ситкома \"Друзья\" ключевое слово \"друг\", конечно, хорошее и мы понимаем, почему оно там (небезосновательно!) появилось, но все-таки это не идеальное ключевое слово.\n",
    "\n",
    "\n",
    "Также мне хотелось бы вообще думать что-то про использование предобученных эбмеддингах трансформеров, например, tiny-rubert, и использовать какой-нибудь Self-Attention слой и таким образом повыделять \"самые важные\" слова в тексте. \n",
    "\n",
    "P.S.:  много анализа и предложений исправления неточностей выдачи алгоритмов я прописала в пунктах 3.1-3.3, чтобы удобно было и анализировать результаты, и смотреть на выдачу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
